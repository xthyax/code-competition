{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and model building for House price on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "\n",
    "def ShowDataGraph(data_frame):\n",
    "\n",
    "    colors = [\"Red\", \"Green\", \"Blue\", \"Orange\", \"Gold\", \"Darkseagreen\"]\n",
    "\n",
    "    len_columns = len(data_frame.columns)\n",
    "\n",
    "    columns_name = list(data_frame.columns)\n",
    "\n",
    "    fig = make_subplots(rows=len_columns//2 + 1, cols=2, subplot_titles=tuple(columns_name))\n",
    "    current_col = 1\n",
    "\n",
    "    for i in columns_name:\n",
    "        if data_frame[i].dtype == \"object\":\n",
    "            fig.add_trace(go.Bar(x=list(dict(data_frame[i].value_counts(sort=False)).keys()) ,y=list(dict(data_frame[i].value_counts(sort=False)).values()) ), row=columns_name.index(i) //2 + 1 , col=current_col)\n",
    "        \n",
    "        else:\n",
    "            fig.add_trace(go.Histogram(x=list(data_frame[i])), row=columns_name.index(i) //2 + 1 , col=current_col)\n",
    "        current_col = current_col + 1 if current_col < 2 else 1\n",
    "            \n",
    "    fig.update_layout(height=200 * len_columns// 2 , width= 900 ,title=\"Feature values\",template=\"plotly_white\", showlegend=False)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"D:\\Coding_practice\\_Data\\Housing_prices_competition\\train.csv\"\n",
    "X_full = pd.read_csv(train_path, index_col='Id')\n",
    "\n",
    "test_path = r\"D:\\Coding_practice\\_Data\\Housing_prices_competition\\test.csv\"\n",
    "X_test_full = pd.read_csv(test_path, index_col='Id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple preprocess\n",
    "- ### Drop NA columns (only drop the column that have len(X_full\\[col_name]) > len(X_full)//4)\n",
    "- ### Drop y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = \"SalePrice\"\n",
    "X_train_full = X_full.copy()\n",
    "X_test = X_test_full.copy()\n",
    "\n",
    "X_train_full.dropna(axis=1, inplace=True, thresh=len(X_full)//4)\n",
    "X_test.dropna(axis=1, inplace=True, thresh=len(X_test_full)//4)\n",
    "\n",
    "y = X_train_full[y_column]\n",
    "X_train_full.drop([y_column], axis=1, inplace=True)\n",
    "print(\"Done\")\n",
    "print(f\"Numerical columns: {len([cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64','float64']])}\")\n",
    "print(f\"Categorical columns: {len([cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['object']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShowDataGraph(X_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buidling pipeline\n",
    "- ### Split Train and Validation\n",
    "- ### Preprocess\n",
    "    - [x] Imputation (for numerical value) \n",
    "        - Median imputation\n",
    "    - [x] Handle categorical variables (Label encoding / One-hot encoding)\n",
    "        - ~~Try with One-hot encoding first~~\n",
    "        - Use Label encoding due to One-hot lead too many features\n",
    "    - [x] Handle outlier\n",
    "    - [] Consider remove leakage data\n",
    "- ### Building model\n",
    "    - [] Simple Random Forest\n",
    "    - [] Build Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "## Using simple Random Forest Regressor as base model to evaluate the preprocess methods\n",
    "def method_evaluate(X, y, method_formulas, method_names, list_estimator, list_random_state):\n",
    "    assert len(method_formulas) == len(method_names)\n",
    "    results = []\n",
    "\n",
    "    for random_state in list_random_state:\n",
    "        print(f\"Running state: {random_state}\")\n",
    "        for n_estimator in list_estimator:\n",
    "            method_scores = []\n",
    "            regressor = RandomForestRegressor(n_estimators=n_estimator, random_state=random_state)\n",
    "\n",
    "            print(f\"The number of trees in the forest: {n_estimator}\")\n",
    "            \n",
    "            for method in method_formulas:\n",
    "                estimator = make_pipeline(method, regressor)\n",
    "                method_score =  -1 * cross_val_score(estimator, X, y, scoring=\"neg_mean_squared_log_error\", cv=5)\n",
    "\n",
    "                method_scores.append(np.sqrt(method_score.mean()))\n",
    "\n",
    "            # for i in range(len(method_names)):\n",
    "            #     print(f\"{method_names[i]}: \\t{method_scores[i]}\")\n",
    "\n",
    "            results.append(np.argmin(np.array(method_scores)))\n",
    "            print(\"-\"*20)\n",
    "\n",
    "    unique_results = list(dict.fromkeys(results))\n",
    "    duplicate_unique = [results.count(i) for i in unique_results]\n",
    "    best_method = unique_results[np.argmax(np.array(duplicate_unique))]\n",
    "\n",
    "    print(f\"{method_names[best_method]} is the best method with {max(duplicate_unique)} time max on {len(list_random_state) * len(list_estimator)}\")\n",
    "\n",
    "## Handle outlier for numerical variables\n",
    "def handle_outlier(data_frame):\n",
    "    numeric_cols = [cname for cname in data_frame.columns if data_frame[cname].dtype in ['int64', 'float64']]\n",
    "    df_ = data_frame.copy()\n",
    "\n",
    "    for cname in numeric_cols:\n",
    "        if len(df_[cname].unique()) > 100:\n",
    "            upper_lim = df_[cname].quantile(.95)\n",
    "            lower_lim = df_[cname].quantile(.05)\n",
    "\n",
    "            df_.loc[(df_[cname] > upper_lim), cname] = upper_lim\n",
    "            df_.loc[(df_[cname] < lower_lim), cname] = lower_lim\n",
    "\n",
    "    return df_\n",
    "\n",
    "## Get mean and standard deviation of everypoints per columns\n",
    "def get_mean_n_std(data_frame):\n",
    "    col_names = [cname for cname in data_frame.columns]\n",
    "    mean_n_std_df = pd.DataFrame()\n",
    "\n",
    "    for cname in col_names:\n",
    "        mean, std = data_frame[cname].mean(), data_frame[cname].std()\n",
    "        mean_n_std_df[cname] = [mean, std]\n",
    "\n",
    "    return mean_n_std_df\n",
    "\n",
    "## Standarize data point every columns in dataframe\n",
    "def get_z_score(data_frame, mean_n_std_frame):\n",
    "    assert len(data_frame.columns) == len(mean_n_std_frame.columns)\n",
    "\n",
    "    col_names = [cname for cname in data_frame.columns]\n",
    "    \n",
    "    standarize_df = data_frame.copy()\n",
    "\n",
    "    for cname in col_names:\n",
    "        \n",
    "        standarize_df[cname] = (standarize_df[cname] - mean_n_std_frame[cname][0]) / mean_n_std_frame[cname][1]\n",
    "\n",
    "    return standarize_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_testing = pd.get_dummies(X_pp)\n",
    "# X_testing\n",
    "# X_testing[\"SaleType_ConLw\"].dtype\n",
    "\n",
    "print(X_train_full[\"LotFrontage\"].mean())\n",
    "print(X_train_full[\"LotFrontage\"].max())\n",
    "print(X_train_full[\"LotFrontage\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess numerical value\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "X_imputation = X_train_full[numeric_cols]\n",
    "\n",
    "imputation_methods =[\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=-1),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    KNNImputer(),\n",
    "    IterativeImputer(random_state=0, n_nearest_features=5)\n",
    "]\n",
    "\n",
    "imputation_names =[\n",
    "    \"Minus Imputation\",\n",
    "    \"Mean Imputation\",\n",
    "    \"Median Imputation\",\n",
    "    \"KNN Imputation\",\n",
    "    \"Iterative Imputation\"\n",
    "]\n",
    "\n",
    "method_evaluate(X_imputation, y, imputation_methods, imputation_names, [200,500,1000,1500], [0,10,100])\n",
    "\n",
    "## Split train and validate\n",
    "## Only use this split to build a final model to predict test set\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_pp, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle numerical data\n",
    "def preprocess_numerical_data(train_data, test_data):\n",
    "\n",
    "    assert list(train_data.columns) == list(test_data.columns)\n",
    "    numerical_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "    X_train = train_data[numerical_cols].copy()\n",
    "    X_test = test_data[numerical_cols].copy()\n",
    "\n",
    "    ## Imputation\n",
    "    imputation = IterativeImputer(random_state=0, n_nearest_features=5)\n",
    "    # imputation = SimpleImputer(strategy=\"median\")\n",
    "    imputed_X_train = pd.DataFrame(imputation.fit_transform(X_train), \n",
    "                        index=pd.RangeIndex(start=1, stop=len(X_train) + 1))\n",
    "\n",
    "    imputed_X_test = pd.DataFrame(imputation.transform(X_test), \n",
    "                        index=pd.RangeIndex(start=min(X_test.index), stop=max(X_test.index)+ 1 ))\n",
    "\n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_test.columns = X_test.columns\n",
    "\n",
    "    ## Outlier\n",
    "    outlier_X_train = handle_outlier(imputed_X_train)\n",
    "    outlier_X_test = handle_outlier(imputed_X_test)\n",
    "    # print(outlier_X_test[\"MSSubClass\"])\n",
    "\n",
    "    ## Scaling\n",
    "    mean_n_std_df = get_mean_n_std(outlier_X_train)\n",
    "    scaled_X_train = get_z_score(outlier_X_train, mean_n_std_df)\n",
    "    scaled_X_test = get_z_score(outlier_X_test, mean_n_std_df)\n",
    "\n",
    "    # print(scaled_X_train)\n",
    "    train_data_frame = train_data.copy()\n",
    "    test_data_frame = test_data.copy()\n",
    "\n",
    "    # print(data_frame)\n",
    "\n",
    "    train_data_frame[numerical_cols] = scaled_X_train[numerical_cols]\n",
    "    test_data_frame[numerical_cols] = scaled_X_test[numerical_cols]\n",
    "\n",
    "    return train_data_frame, test_data_frame\n",
    "\n",
    "## Handle categorical data\n",
    "def preprocess_categorical_data(train_data_frame, test_data_frame):\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    train_df = train_data_frame.copy()\n",
    "    test_df = test_data_frame.copy()\n",
    "\n",
    "    categorical_cols = [cname for cname in train_data_frame.columns if train_data_frame[cname].dtype in ['object']]\n",
    "    # ## One-hot encoding\n",
    "    # data_frame = pd.get_dummies(df_)\n",
    "    # Label encoding\n",
    "    for cname in categorical_cols:\n",
    "        # print(df_[cname])\n",
    "        column = pd.concat([train_df[cname], test_df[cname]])\n",
    "        if train_df[cname].isna().values.any():\n",
    "            print(cname)\n",
    "\n",
    "        codes, unique = column.factorize(sort=True)\n",
    "        train_df[cname] = codes[min(train_df.index) - 1: max(train_df.index)]\n",
    "        test_df[cname] = codes[min(test_df.index) - 1: max(test_df.index)]\n",
    "        # print(len(codes[min(train_df.index) - 1: max(train_df.index)]))\n",
    "        # break\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numerical, X_test_numerical = preprocess_numerical_data(X_train_full, X_test)\n",
    "X_pp, X_test_pp = preprocess_categorical_data(X_train_numerical, X_test_numerical)\n",
    "# print(X_temp)\n",
    "# print(X_pp)\n",
    "# print(X_test_numerical[\"MSSubClass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pp.isna().stack()[lambda x:x].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "- ### [] Using Cross-Validation to choose parameters for model\n",
    "    - Choose the best ensemble models\n",
    "    - Tune the models with right hyper parameters\n",
    "- ### [] Export submission file on Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Names =[\n",
    "    \"Bagging Regressor\",\n",
    "    \"Random Forest Regressor\",\n",
    "    \"Gradient Boosting Regressor\",\n",
    "    \"Ada Boost Regressor\",\n",
    "    \"Extra Trees Regressor\",\n",
    "    \"XGB Regressor\",\n",
    "    \"Voting Regressor\"\n",
    "]\n",
    "\n",
    "my_Model = [\n",
    "     BaggingRegressor(DecisionTreeRegressor(criterion=\"mae\", min_samples_split=12),  n_estimators=1000, max_samples=0.8, max_features=1.0, random_state=0),\n",
    "     RandomForestRegressor(n_estimators=1000, criterion=\"mae\", min_samples_split=12, max_samples=0.8, random_state=0),\n",
    "     GradientBoostingRegressor(learning_rate=0.1, n_estimators=1000, criterion=\"mae\",min_samples_split=12, random_state=0),\n",
    "     AdaBoostRegressor(DecisionTreeRegressor(criterion='mae', min_samples_split=12), n_estimators=1000, learning_rate=1.0, random_state=0),\n",
    "     ExtraTreesRegressor(n_estimators=1000, criterion=\"mae\", min_samples_split=12, max_samples=0.8, random_state=0),\n",
    "     XGBRegressor(n_estimators=200, random_state=0, learning_rate=0.3, tree_method='gpu_hist', gpu_id=0, gamma=10)\n",
    "]\n",
    "list_model = [(str(i), model) for i, model in enumerate(my_Model)]\n",
    "ensemble_regressor = VotingRegressor(list_model)\n",
    "\n",
    "my_Model.append(ensemble_regressor)\n",
    "\n",
    "print(\"Mean_cross_validate:\\n\")\n",
    "assert len(model_Names) == len(my_Model)\n",
    "\n",
    "for i, regressor in enumerate(my_Model):\n",
    "    scores = -1 * cross_val_score(regressor, X_pp, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    print(f\"{model_Names[i]}: \\t{np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict_model = GradientBoostingRegressor(learning_rate=0.1, n_estimators=1000, criterion=\"mae\",min_samples_split=12, random_state=0)\n",
    "my_Model = [\n",
    "     BaggingRegressor(DecisionTreeRegressor(criterion=\"mse\", min_samples_split=12),  n_estimators=1000, max_samples=0.8, max_features=1.0, random_state=0),\n",
    "     RandomForestRegressor(n_estimators=1000, criterion=\"mse\", min_samples_split=12, max_samples=0.8, random_state=0),\n",
    "     GradientBoostingRegressor(learning_rate=0.1, n_estimators=1000, criterion=\"mse\",min_samples_split=12, random_state=0),\n",
    "     AdaBoostRegressor(DecisionTreeRegressor(criterion='mse', min_samples_split=12), n_estimators=1000, learning_rate=1.0, random_state=0),\n",
    "     ExtraTreesRegressor(n_estimators=1000, criterion=\"mse\", min_samples_split=12, max_samples=0.8, random_state=0),\n",
    "     XGBRegressor(n_estimators=200, random_state=0, learning_rate=0.3, tree_method='gpu_hist', gpu_id=0, gamma=10)\n",
    "]\n",
    "list_model = [(str(i), model) for i, model in enumerate(my_Model)]\n",
    "Predict_model = VotingRegressor(list_model)\n",
    "Predict_model.fit(X_pp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = Predict_model.predict(X_test_pp)\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'Id': X_test.index,\n",
    "    'Saleprice': preds_test\n",
    "})\n",
    "\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bit5a1d938943a948eaba7cedb71c9a2197",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}